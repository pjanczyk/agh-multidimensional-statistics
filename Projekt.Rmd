---
title: "AGH | Statystyka Wielowymiarowa | Projekt"
author: "Piotr Janczyk, Miłosz Janowski"
output: html_document
---

# AGH | Statystyka Wielowymiarowa | Projekt

[Piotr Janczyk](https://github.com/pjanczyk), [Miłosz Janowski](https://github.com/milekj)

Analiza zbioru danych [diamonds](https://github.com/vincentarelbundock/Rdatasets/blob/master/csv/ggplot2/diamonds.csv)

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = '', echo = TRUE)
set.seed(1)
```

## Zbiór danych

Cechy:  

* `price` — price in US dollars (\$326–\$18,823)
* `carat` — weight of the diamond (0.2–5.01)
* `cut` — quality of the cut (_Fair_, _Good_, _Very Good_, _Premium_, _Ideal_)
* `color` — diamond colour, from _D_ (best) to _J_ (worst)
* `clarity` — a measurement of how clear the diamond is (_I1_ (worst), _SI2_, _SI1_, _VS2_, _VS1_, _VVS2_, _VVS1_, _IF_ (best))
* `x` — length in mm (0–10.74)
* `y` — width in mm (0–58.9)
* `z` — depth in mm (0–31.8)
* `depth` — total depth percentage = `z / mean(x, y)` = `2 * z / (x + y)` (43–79)
* `table` — width of top of diamond relative to widest point (43–95)


```{r}
diamonds <- read.csv("diamonds.csv", header = TRUE, stringsAsFactors = TRUE)
str(diamonds)
knitr::kable(head(diamonds))
```


## Przygotowanie zbioru danych
```{r}
# Usunięcie liczb porządkowych
diamonds$X <- NULL

# Odfitrowanie kilkunastu rekordów z brakującymi danymi
diamonds <- diamonds[diamonds$x > 0 & diamonds$y > 0 & diamonds$z > 0,]

# Posortowanie zmiennych typu factor
diamonds$color <- factor(diamonds$color,
                         levels = c("D", "E", "F", "G", "H", "I", "J"),
                         ordered = TRUE)

diamonds$clarity <- factor(diamonds$clarity,
                           levels = c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"),
                           ordered = TRUE)

diamonds$cut <- factor(diamonds$cut,
                       levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"),
                       ordered = TRUE)

# Konwersja do zmiennych numerycznych
diamonds_num <- transform(diamonds,
                          color = as.numeric(color),
                          clarity = as.numeric(clarity),
                          cut = as.numeric(cut))
```

## Podział na zbiór treningowy i testowy

```{r}
n <- nrow(diamonds)
train <- sample(1:n, n / 2)
test <- -train
```

## Pomocnicze funkcje

```{r}
fancy_plot <- function (x, y, ...) {
  if (is.numeric(x)) {
    # Powiększenie danych na wykresie poprzez ukrycie daleko oddalonych skrajnych punktów
    xlim = quantile(x, c(0.001, 0.999))
    col = "#666666"
  } else {
    xlim = NULL
    col = "#cccccc"
  }

  plot(x = x, y = y, xlim = xlim, pch = 20, cex = 0.05, col = col, ...)
}
```


## Regresja jednokrotna

Regresja liniowa, regresja wielomianowa, wygładzające funkcje sklejane

```{r, warning=FALSE}
single_regression <- function (predictor, title, xlab, show_summary = TRUE) {
  fancy_plot(x = predictor, y = diamonds$price,
             main = title,
             xlab = xlab,
             ylab = "price [$]")
  
  predictor_num <- as.numeric(predictor)
  
  # Regresja liniowa
  linear_fit <- lm(diamonds$price ~ predictor_num, subset = train)
  abline(linear_fit, lwd = 2, col = "darkgreen", lty = 2)
  
  # Regresja wielomianowa
  poly_fit <- lm(diamonds$price ~ poly(predictor_num, 3), subset = train)
  grid <- seq(min(predictor_num), max(predictor_num), length.out = 100)
  lines(x = grid, y = predict(poly_fit, list(predictor_num = grid), se.fit = TRUE)$fit,
        lwd = 2, col = "blue")
  
  # Wygładzające funkcje sklejane
  spline_fit <- smooth.spline(predictor_num[train], diamonds$price[train], cv = TRUE)
  lines(spline_fit, col = "red", lwd = 2)
  
  legend("topleft",
         legend=c("lin.", "wiel.", "w.f.s."),
         col=c("darkgreen", "blue", "red"),
         lty=c(2, 1, 1),
         lwd = 2)
  
  linear_pred = predict(linear_fit, list(predictor_num = predictor_num[test]))
  poly_pred = predict(poly_fit, list(predictor_num = predictor_num[test]))
  linear_mse = mean((diamonds$price[test] - linear_pred) ^ 2)
  poly_mse = mean((diamonds$price[test] - poly_pred) ^ 2)
  
  if (show_summary) {
    print(knitr::kable(cbind(
      linear_mse = linear_mse,
      poly_mse = poly_mse)))

    print(summary(linear_fit))
    print(summary(poly_fit))
  }
}

single_regression(diamonds$carat, "price / carat", "carat [ct]")
single_regression(diamonds$cut, "price / cut", "cut")
single_regression(diamonds$color, "price / color", "color")
single_regression(diamonds$clarity, "price / clarity", "clarity")
single_regression(diamonds$depth, "price / depth", "depth [%]")
single_regression(diamonds$depth + runif(nrow(diamonds), -0.05, 0.05),
                  "price / depth", "depth [%] (z dodatkiem losowego szumu)",
                  show_summary = FALSE)
single_regression(diamonds$table, "price / table", "table")
single_regression(as.factor(round(diamonds$table)),
                  "price / table",
                  "table (zakrąglone do najbliższej liczby całkowitej)",
                  show_summary = FALSE)
single_regression(diamonds$x, "price / x", "x [mm]")
single_regression(diamonds$y, "price / y", "y [mm]")
single_regression(diamonds$z, "price / z", "z [mm]")
```

## Regresja liniowa wielokrotna

```{r}
lm_fit <- lm(price ~ ., data = diamonds_num, subset = train)
summary(lm_fit)

lm_pred <- predict(lm_fit, diamonds_num[test,])

lm_mse <- mean((diamonds_num$price[test] - lm_pred) ^ 2)
lm_mse
```

## Wybór najlepszego podzbioru cech

```{r}
library(leaps)
bs_fit <- regsubsets(price ~ ., data = diamonds_num, nvmax = Inf, subset = train)
bs_fit_summary <- summary(bs_fit)
bs_fit_summary
```

#### Kryterium BIC

```{r}
plot(bs_fit, scale = 'bic')
```

#### Przy pomocy metody zbioru walidacyjnego

```{r}
predict.regsubsets <- function(object, newdata, id, ...) {
  model.formula <- as.formula(object$call[[2]])
  mat <- model.matrix(model.formula, newdata)
  coefs <- coef(object, id = id)
  mat[, names(coefs)] %*% coefs
}

bs_mse <- sapply(1:9, function(i) {
  pred <- predict(bs_fit, diamonds_num[test,], id = i)
  mean((diamonds_num$price[test] - pred) ^ 2)
})
bs_mse

plot(bs_mse, 
     xlab = "Liczba zmiennych", ylab = "MSE",
     col = "black", type = "b", pch = 20)
points(which.min(bs_mse), min(bs_mse), col = "red", pch = 9)
```

## Drzewo decyzyjne
```{r}
library(tree)
Cost <- factor(ifelse(diamonds$price <= 1000, "Cheap", ifelse(diamonds$price <= 10000, "Moderate", "Expansive")))
diamondsE <- data.frame(diamonds_num, Cost)

diamonds.tree <- tree(Cost ~ . - price, data = diamondsE, subset = train)
plot(diamonds.tree)
text(diamonds.tree, pretty = 0)
summary(diamonds.tree)

tree.class <- predict(diamonds.tree, newdata = diamondsE[test,], type = "class")
table(tree.class, diamondsE$Cost[test])
mean(tree.class != diamondsE$Cost[test])
```

## Regresja logistyczna GAM
```{r, warning=FALSE}
library(gam)
fit.gam.bf <- gam(I(price > 10000) ~ s(table, df = 5) + cut + clarity + color, data = diamonds, family = binomial)
summary(fit.gam.bf)
plot(fit.gam.bf, col = "red")
```

