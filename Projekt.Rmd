---
title: "Statystyka Wielowymiarowa — Projekt"
author: "Piotr Janczyk, Miłosz Janowski"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = '', echo = TRUE)
```

```{r}
library(leaps)
set.seed(1)
```


# Zbiór danych

Cechy:  
* `price` — price in US dollars (\$326–\$18,823)
* `carat` — weight of the diamond (0.2–5.01)
* `cut` — quality of the cut (_Fair_, _Good_, _Very Good_, _Premium_, _Ideal_)
* `color` — diamond colour, from _D_ (best) to _J_ (worst)
* `clarity` — a measurement of how clear the diamond is (_I1_ (worst), _SI2_, _SI1_, _VS2_, _VS1_, _VVS2_, _VVS1_, _IF_ (best))
* `x` — length in mm (0–10.74)
* `y` — width in mm (0–58.9)
* `z` — depth in mm (0–31.8)
* `depth` — total depth percentage = `z / mean(x, y)` = `2 * z / (x + y)` (43–79)
* `table` — width of top of diamond relative to widest point (43–95)

```{r}
diamonds <- read.csv("diamonds.csv", header = TRUE, stringsAsFactors = TRUE)
diamondsG <- diamonds
str(diamonds)
head(diamonds)
```


# Przygotowanie zbioru danych
```{r}
# Usunięcie liczb porządkowych
diamonds$X <- NULL

# Konwersja zmiennych typu factor na liczby
diamonds$color <- factor(diamonds$color,
                        levels = c("D", "E", "F", "G", "H", "I", "J"),
                        ordered = TRUE)
diamonds$color <- as.numeric(diamonds$color)

diamonds$clarity <- factor(diamonds$clarity,
                          levels = c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"),
                          ordered = TRUE)
diamonds$clarity <- as.numeric(diamonds$clarity)

diamonds$cut <- factor(diamonds$cut,
                      levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"),
                      ordered = TRUE)
diamonds$cut <- as.numeric(diamonds$cut)
```

# Podział na zbiór treningowy i testowy

```{r}
n <- nrow(diamonds)
train <- sample(1:n, n / 2)
test <- -train
```

# Regresja liniowa

```{r}
lm_fit <- lm(price ~ ., data = diamonds, subset = train)
summary(lm_fit)

lm_pred <- predict(lm_fit, diamonds[test,])

lm_mse <- mean((diamonds$price[test] - lm_pred) ^ 2)
lm_mse
```

# Wybór najlepszego podzbioru cech

```{r}
bs_fit <- regsubsets(price ~ ., data = diamonds, nvmax = Inf)
bs_fit_summary <- summary(bs_fit)
bs_fit_summary
```

### Kryterium BIC

```{r}
plot(bs_fit, scale = 'bic')
```

### Przy pomocy metody zbioru walidacyjnego

```{r}
predict.regsubsets <- function(object, newdata, id, ...) {
  model.formula <- as.formula(object$call[[2]])
  mat <- model.matrix(model.formula, newdata)
  coefs <- coef(object, id = id)
  mat[, names(coefs)] %*% coefs
}

bs_mse <- sapply(1:9, function(i) {
  pred <- predict(bs_fit, diamonds[test,], id = i)
  mean((diamonds$price[test] - pred) ^ 2)
})
bs_mse

plot(bs_mse, 
     xlab = "Liczba zmiennych", ylab = "MSE",
     col = "black", type = "b", pch = 20)
points(which.min(bs_mse), min(bs_mse), col = "red", pch = 9)
```

# Drzewo decyzyjne
```{r}
library(tree)
Cost <- factor(ifelse(diamonds$price <= 1000, "Cheap", ifelse(diamonds$price <= 10000, "Moderate", "Expansive")))
diamondsE <- data.frame(diamonds, Cost)

diamonds.tree <- tree(Cost ~ . - price, data = diamondsE, subset = train)
plot(diamonds.tree)
text(diamonds.tree, pretty = 0)
summary(diamonds.tree)

tree.class <- predict(diamonds.tree, newdata = diamondsE[test,], type = "class")
table(tree.class, diamondsE$Cost[test])
mean(tree.class != diamondsE$Cost[test])

```

# Regresja wielomianowa i naturalnymi funkcjami sklejanymi
```{r}
diamondsX <- diamonds[diamonds$x > 0, ]
fit.poly <- lm(price ~ poly(x, 3), data = diamondsX)
summary(fit.poly)

x.lims <- range(diamondsX$x)
x.grid <- seq(x.lims[1], x.lims[2])
pred.poly <- predict(fit.poly, list(x = x.grid), se.fit = TRUE)
se.bands <- cbind(pred.poly$fit + 2 * pred.poly$se.fit, 
                  pred.poly$fit - 2 * pred.poly$se.fit)
fit.smooth.cv <- smooth.spline(diamondsX$x, diamondsX$price)

plot(diamondsX$x, diamondsX$price, col = "darkgrey", cex = 0.5, xlim = x.lims)
lines(x.grid, pred.poly$fit, col = "red", lwd = 2)
matlines(x.grid, se.bands, col = "red", lty = "dashed")
lines(fit.smooth.cv, col = "blue", lwd = 2)
```

# Regresja logistyczna GAM
```{r}
diamondsG$cut <- factor(diamondsG$cut, levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
diamondsG$clarity <- factor(diamondsG$clarity, levels = c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))

library(gam)
fit.gam.bf <- gam(I(price > 10000) ~ s(table, df = 5) + cut + clarity + color, data = diamondsG, family = binomial)
summary(fit.gam.bf)
#par(mfrow = c(1,4))
plot(fit.gam.bf, col = "red")
```

